{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1822a7d-5698-4f4d-92ec-7f712fac9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6831f6f9-c45e-4e48-82e9-e8f9b02d207f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1816afb0-2565-4fbd-917d-54c4b391fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据数量260386\n",
      "                                             content  stars  label\n",
      "0  “我相信真正纯正的爱情能产生一个纾解死亡的阶段，所有的懦弱都出自于没有爱或爱得不彻底，这两者...      4      1\n",
      "1  太现实不是女人的错，不过年老色衰、中年危机了就不要自以为是，幻想重新寻找当年一口拒绝了的、虽...      4      1\n",
      "2                               跑吧，我们无力对抗，但也不能让他们得逞。      5      1\n",
      "3  我在同样变态的师傅手下呆了三年，祖宗十八代被骂了个遍，没空吃饭上厕所睡觉交朋友谈恋爱，脊椎侧...      5      1\n",
      "4                    还可以，是比较好的电影，但是又觉得和吕克贝松的巅峰状态差了好多      4      1\n"
     ]
    }
   ],
   "source": [
    "# 读取数据集\n",
    "data_path = '../datasets/chinese_movie_reviews/chinese_movie_reviews_datasets.jsonl'\n",
    "df = pd.read_json(data_path, orient='records', lines=True)\n",
    "print(f'数据数量{len(df)}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd708c-5e3a-487b-a8b9-026f91c9b685",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "1. 分词\n",
    "2. 训练Word2Vec：生成一个包含语料库中的每个词的向量空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da13e00-6a47-453a-8faf-fd551b5c8ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.301 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大文本长度：166\n",
      "平均文本长度：21.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>stars</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118816</th>\n",
       "      <td>星星 青青 子 衿 悠悠 我心 但 为 君故 沉吟 至今 一日不见 如 三月 兮 求之不得 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250417</th>\n",
       "      <td>想 看 多年 片子 今天 看 完 之后 却 有些 失望 剧情 想象 美好 甚至 根本 谈不上...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181970</th>\n",
       "      <td>女 领导 说 郭达 像 院子 没 拴住 狗 还 到处 拉屎 乱搞 太 形象 哈哈 库珀 左拳...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246242</th>\n",
       "      <td>这片 居然 豆瓣 额 哈哈哈哈 哈哈哈哈 人间 真是 不可 推测 灰色 喜剧</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186430</th>\n",
       "      <td>完全 没 剧情 画面 不错</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  content  stars  label\n",
       "118816  星星 青青 子 衿 悠悠 我心 但 为 君故 沉吟 至今 一日不见 如 三月 兮 求之不得 ...      4      1\n",
       "250417  想 看 多年 片子 今天 看 完 之后 却 有些 失望 剧情 想象 美好 甚至 根本 谈不上...      3      0\n",
       "181970  女 领导 说 郭达 像 院子 没 拴住 狗 还 到处 拉屎 乱搞 太 形象 哈哈 库珀 左拳...      0      0\n",
       "246242             这片 居然 豆瓣 额 哈哈哈哈 哈哈哈哈 人间 真是 不可 推测 灰色 喜剧      1      0\n",
       "186430                                      完全 没 剧情 画面 不错      3      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取停用词表\n",
    "with open('../datasets/chinese_movie_reviews/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 移除数字\n",
    "def remove_digit(text):\n",
    "     return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 分词处理\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "# 应用预处理\n",
    "def process_row(text):\n",
    "    text = remove_digit(text)\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fa5]\", \"\", text)  # 只保留汉字字符\n",
    "    text = tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(process_row) # 作者在这里大概用了2分钟\n",
    "\n",
    "# 计算每条文本的长度\n",
    "sentence_lengths = df[\"content\"].apply(lambda x: len(x.split()))  # 计算每条文本的词数（已经分词）\n",
    "# 计算最大长度和平均长度\n",
    "max_length = sentence_lengths.max()\n",
    "avg_length = sentence_lengths.mean()\n",
    "print(f\"最大文本长度：{max_length}\")\n",
    "print(f\"平均文本长度：{avg_length:.2f}\")\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0beb8-daab-4f56-bd8e-ca6391ad74d0",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "`Word2Vec训练很快，一分钟以内就能结束`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c737eff-6d74-4c26-9acb-fcdd69cb0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def prepare_data(df, vector_size, max_length=100):\n",
    "    # 将文本转换为词列表\n",
    "    texts = df['content'].apply(lambda x: x.split())\n",
    "\n",
    "    # 首先创建一个空的词汇表并添加 <PAD> 词索引为 0\n",
    "    vocab = {\"<PAD>\": 0}\n",
    "    \n",
    "    # 训练Word2Vec模型\n",
    "    w2v_model = Word2Vec(sentences=texts, vector_size=vector_size, window=8, min_count=1, workers=4)\n",
    "    \n",
    "    # 获取模型训练后生成的词汇表\n",
    "    vocab.update({k: v+1 for k, v in w2v_model.wv.key_to_index.items()})\n",
    "    \n",
    "    # 将文本转换为序列，如果词不在词汇表中，则用0表示\n",
    "    sequences = [[vocab.get(word, 0) for word in text] for text in texts]\n",
    "\n",
    "    # 对每个序列进行填充或截断\n",
    "    padded = [s[:max_length] + [0] * (max_length - len(s)) if len(s) < max_length \n",
    "              else s[:max_length] for s in sequences]\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(vocab), vector_size))\n",
    "    for word, i in vocab.items():\n",
    "        if word != \"<PAD>\":  # 确保 <PAD> 不会被赋予任何词向量\n",
    "            embedding_matrix[i] = w2v_model.wv[word]\n",
    "    \n",
    "    return np.array(padded), embedding_matrix, vocab\n",
    "\n",
    "vector_size=256\n",
    "padded, embedding_matrix, vocab = prepare_data(df, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872a2c11-6a54-4485-89c9-006af96d0524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小： 62221\n",
      "词汇表的一部分:\n",
      "词: <PAD>, 索引: 0\n",
      "词: 看, 索引: 1\n",
      "词: 人, 索引: 2\n",
      "词: 但, 索引: 3\n",
      "词: 好, 索引: 4\n",
      "狗 的词向量:\n",
      "[-2.36567736e-01 -1.81298125e+00  1.21999574e+00  5.98021567e-01\n",
      " -7.45063961e-01 -9.00016546e-01  1.39530659e+00  8.75249505e-02\n",
      "  9.44862217e-02 -2.27183059e-01  8.02466094e-01  4.17786032e-01\n",
      " -1.23330700e+00 -1.26467597e+00 -1.03314006e+00  2.52983034e-01\n",
      " -1.06839933e-01 -1.45807362e+00 -2.17082933e-01  1.82238653e-01\n",
      "  1.27316415e+00  1.82120705e+00  2.78290063e-01  8.53154004e-01\n",
      " -8.26691613e-02  1.88696980e-01 -1.55740094e+00 -4.38516945e-01\n",
      "  1.26891887e+00 -7.75177002e-01 -2.30401799e-01  1.53721225e+00\n",
      "  3.56354192e-02 -1.19395781e+00 -5.02536178e-01  6.80065900e-02\n",
      "  1.05687715e-01  6.16707563e-01  7.03328133e-01  6.87454417e-02\n",
      " -1.48744419e-01  6.16866827e-01  3.77114773e-01 -3.70331675e-01\n",
      "  1.41153842e-01  6.74931824e-01  2.14020729e-01  6.24394007e-02\n",
      " -1.70209959e-01  6.99220151e-02  5.40796697e-01 -8.27411283e-03\n",
      " -4.45683390e-01 -1.78815514e-01 -4.80320573e-01 -3.56745511e-01\n",
      " -1.79851558e-02  1.75096527e-01  6.95033550e-01  9.81706440e-01\n",
      " -3.07979714e-02 -3.10334712e-01  3.15292150e-01 -2.26925299e-01\n",
      " -7.89403915e-01 -4.13274080e-01 -5.45371830e-01  3.50906730e-01\n",
      " -3.18455040e-01 -5.04791141e-02 -7.76221529e-02  8.38443935e-01\n",
      " -9.43166256e-01 -8.36871564e-01 -1.24708164e+00 -2.12009534e-01\n",
      "  1.62667364e-01 -6.37567282e-01 -1.29439294e+00  6.78695142e-01\n",
      "  8.10559750e-01 -1.59170374e-01  1.63639560e-01  8.25319171e-01\n",
      "  6.97574615e-01  1.27023017e+00  6.15272164e-01  2.38837808e-01\n",
      " -1.25242099e-01  9.88852561e-01 -1.02953553e-01 -5.83501637e-01\n",
      " -2.01980948e+00 -1.11951756e+00  1.73459351e+00  1.62495041e+00\n",
      "  1.70527589e+00  1.37944492e-02 -1.49571583e-01  9.74154949e-01\n",
      "  7.97349691e-01  1.73783934e+00 -3.35310727e-01 -5.07829547e-01\n",
      " -8.04023027e-01 -8.34862590e-01 -1.12182701e+00 -3.74394536e-01\n",
      " -6.07486546e-01 -7.32738316e-01  1.67932463e+00  5.21848559e-01\n",
      "  1.12758088e+00  1.68075040e-01 -2.54616678e-01 -2.53047645e-01\n",
      " -5.58147013e-01  9.69973803e-01 -4.12596911e-01 -9.95959342e-01\n",
      " -3.57599169e-01 -5.70271194e-01 -2.57315561e-02  3.96427304e-01\n",
      " -1.01314020e+00 -8.06226611e-01  2.54201561e-01 -4.77406234e-01\n",
      "  1.33977735e+00 -4.45580006e-01  8.11747387e-02 -9.14176226e-01\n",
      "  9.12008822e-01  1.57332003e-01  1.62055731e-01  1.10759819e+00\n",
      "  5.84558904e-01  1.23780727e+00  1.64510310e-01  5.65208673e-01\n",
      "  3.45108360e-01 -7.34509110e-01  5.67812324e-01  3.10468534e-03\n",
      " -6.74767077e-01  3.04070711e-01 -1.46730626e+00 -3.40898568e-03\n",
      "  6.66820049e-01  6.90581799e-01  2.06180066e-01  8.08782279e-01\n",
      "  4.79948729e-01  4.34795134e-02 -4.75643635e-01  1.21695292e+00\n",
      " -5.09466648e-01  1.10270405e+00 -1.05589807e+00  8.01429212e-01\n",
      " -8.12200665e-01  1.14170599e+00  3.63296062e-01  3.58824506e-02\n",
      "  2.20195323e-01 -1.09126759e+00 -3.05988848e-01 -1.75677538e-01\n",
      " -1.21193158e-03  6.37121201e-01  6.41724348e-01  1.62312582e-01\n",
      " -1.23697007e+00  2.90915757e-01  6.49889588e-01  4.47057188e-01\n",
      " -1.45935559e+00 -9.53727841e-01  9.09272254e-01 -1.68622112e+00\n",
      " -4.15838897e-01 -2.48536989e-01  9.78974879e-01  1.21136773e+00\n",
      "  8.74046206e-01 -7.18813576e-03  6.30458891e-01  3.46996903e-01\n",
      " -9.72023726e-01  7.77445972e-01 -6.43202126e-01 -5.72530627e-01\n",
      " -6.99027538e-01 -5.09554029e-01 -1.80023372e-01 -4.20812756e-01\n",
      " -1.31734943e+00 -2.04947114e+00 -8.17101970e-02 -1.80461180e+00\n",
      " -8.19699407e-01  4.67256099e-01 -1.25741875e+00 -1.25071299e+00\n",
      "  1.36797702e+00  2.37502769e-01 -4.78997439e-01 -1.25028098e+00\n",
      "  4.78104055e-01 -2.76624531e-01  5.48527777e-01  8.59788835e-01\n",
      "  1.84388205e-01  7.44790584e-03  7.84834445e-01  9.17629778e-01\n",
      "  4.02033448e-01 -3.80523473e-01 -2.07976794e+00  3.81224811e-01\n",
      " -1.11760187e+00  1.11744501e-01 -2.79048216e-02  1.31008303e+00\n",
      " -8.18235695e-01  9.01043952e-01  5.42992353e-01 -2.58756399e-01\n",
      " -5.25207579e-01  4.68594462e-01 -7.00497448e-01  7.02507019e-01\n",
      " -1.42363369e+00  9.84466255e-01  1.98797315e-01  1.05549645e+00\n",
      " -6.60156429e-01 -8.15709710e-01  7.85413921e-01 -8.73604715e-01\n",
      " -2.37742758e+00  5.54703176e-01 -3.98644090e-01  7.93705463e-01\n",
      " -6.79935873e-01 -4.09327984e-01  1.22053361e+00 -1.15917444e+00\n",
      "  7.47652173e-01 -2.10570669e+00 -6.11658037e-01  3.54021907e-01\n",
      " -1.27502382e+00 -3.85748684e-01 -2.13464797e-01 -3.47732604e-01]\n"
     ]
    }
   ],
   "source": [
    "# 查看词汇表的前几个词\n",
    "print(f'词汇表大小： {len(vocab)}')\n",
    "print(\"词汇表的一部分:\")\n",
    "for word, idx in list(vocab.items())[:5]:  # 查看前5个词\n",
    "    print(f\"词: {word}, 索引: {idx}\")\n",
    "\n",
    "# 查看嵌入矩阵中对应某个词的词向量\n",
    "word_to_check = '狗'  \n",
    "if word_to_check in vocab:\n",
    "    word_index = vocab[word_to_check]\n",
    "    word_vector = embedding_matrix[word_index]\n",
    "    print(f\"{word_to_check} 的词向量:\")\n",
    "    print(word_vector)\n",
    "else:\n",
    "    print(f\"词汇表中没有 {word_to_check} 这个词。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ae4b5-f74a-4aae-a3fa-b3b6fcee0fe2",
   "metadata": {},
   "source": [
    "#### 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d798fe2c-4a7b-40c6-853e-659d2fcc8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "X = padded\n",
    "y = df['label'].values\n",
    "# stratify=df[\"label\"] 使得训练集和测试集中的标签分布是均匀\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df[\"label\"])  \n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f5b357-af62-4af1-b467-f71fd1c5ed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 234347\n",
      "测试集大小: 26039\n",
      "训练集标签分布: Counter({1: 117174, 0: 117173})\n",
      "测试集标签分布: Counter({0: 13020, 1: 13019})\n"
     ]
    }
   ],
   "source": [
    "# 查看训练集和测试集的大小\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")\n",
    "\n",
    "# 查看训练集和测试集的标签分布\n",
    "from collections import Counter\n",
    "\n",
    "train_labels_counter = Counter(y_train)\n",
    "test_labels_counter = Counter(y_test)\n",
    "print(f\"训练集标签分布: {train_labels_counter}\")\n",
    "print(f\"测试集标签分布: {test_labels_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d58254-08f2-43fc-a87e-23b4943cfafb",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8559dadf-c087-4650-a68c-cee6efcfeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # 定义词嵌入层，使用 embedding_matrix 初始化\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = True # 确保嵌入层的参数可训练\n",
    "        \"\"\"\n",
    "        双向 LSTM 层：输入维度为 embedding_dim，输出维度为 hidden_dim。\n",
    "        batch_first=True : 输入张量的形状为 (batch_size, sequence_length)。\n",
    "        bidirectional=True : LSTM 会在两个方向上（正向和反向）处理输入序列，以捕捉更多上下文信息\n",
    "        (因为 LSTM 是双向的，它的输出将是两个隐藏层的连接, 所以实际输出维度为 hidden_dim * 2)\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,    # 输入特征的维度\n",
    "            hidden_dim,       # 隐藏状态的维度\n",
    "            num_layers=2,     # LSTM的层数\n",
    "            batch_first=True, # 输入和输出的张量的第一个维度是batch_size\n",
    "            bidirectional=True, # 使用双向LSTM\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # 2 classes for binary classification\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded) # 第二个返回值_是LSTM的隐藏状态和单元状态；lstm_out形状： (batch_size, sequence_length, hidden_dim * 2)\n",
    "        last_hidden = lstm_out[:, -1, :] # 选择每个批次中的最后一个时刻的输出，形状为 (batch_size, hidden_dim * 2)\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        fc1_out = F.relu(self.fc1(dropped))\n",
    "        # return self.fc2(fc1_out)\n",
    "        return self.fc2(self.dropout(fc1_out))\n",
    "\n",
    "\n",
    "model = TextClassifier(len(vocab) + 1, vector_size, embedding_matrix).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 初始化学习率调度器，每10个epoch将学习率衰减为原来的gamma倍\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c7826e-5f40-474f-a02a-1d412d87c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (embedding): Embedding(62221, 256, padding_idx=0)\n",
      "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "模型总参数数量: 18,690,050\n",
      "模型可训练参数数量: 18,690,050\n"
     ]
    }
   ],
   "source": [
    "# 查看模型结构\n",
    "# 打印模型参数总数和可训练参数总数\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())  # 所有参数数量\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  # 需要训练的参数数量\n",
    "    print(f\"模型总参数数量: {total_params:,}\")\n",
    "    print(f\"模型可训练参数数量: {trainable_params:,}\")\n",
    "\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0e293-831e-4ace-8ab3-981be5be48f8",
   "metadata": {},
   "source": [
    "## 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdbfde3d-bcc5-4bbc-b954-d3e576d1cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 训练函数\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 使用 tqdm 包裹数据加载器，显示进度条\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for texts, labels in progress_bar:\n",
    "        # 将数据移动到设备\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(texts)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  # 更新模型参数\n",
    "\n",
    "        # 统计指标\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # 更新进度条描述\n",
    "        progress_bar.set_postfix(lr=optimizer.param_groups[0]['lr'], loss=loss.item())\n",
    "        \n",
    "    scheduler.step()  # 更新学习率       \n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36267fd1-6b5e-4a5e-b9a7-e97652a9948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for texts, labels in progress_bar:\n",
    "            # 将数据移动到设备\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(texts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # 统计指标\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # 更新进度条描述\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d90b49-560f-40b2-a1cd-d2fe798c8910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_acc:51.1%, Train_loss:0.693, Test_acc:50.3%，Test_loss:0.692\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train_acc:59.6%, Train_loss:0.651, Test_acc:67.4%，Test_loss:0.604\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train_acc:71.2%, Train_loss:0.554, Test_acc:68.4%，Test_loss:0.590\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train_acc:75.1%, Train_loss:0.495, Test_acc:68.0%，Test_loss:0.595\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train_acc:80.4%, Train_loss:0.405, Test_acc:67.1%，Test_loss:0.695\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train_acc:83.5%, Train_loss:0.344, Test_acc:66.1%，Test_loss:0.837\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "test_loss  = []\n",
    "test_acc   = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    epoch_train_loss, epoch_train_acc = train(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "    # 在测试集上评估\n",
    "    epoch_test_loss, epoch_test_acc = evaluate(test_loader, model, loss_fn)\n",
    "\n",
    "    train_acc.append(epoch_train_acc)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    test_acc.append(epoch_test_acc)\n",
    "    test_loss.append(epoch_test_loss)\n",
    "    # 打印训练和测试结果\n",
    "    template = ('Epoch:{:2d}, Train_acc:{:.1f}%, Train_loss:{:.3f}, Test_acc:{:.1f}%，Test_loss:{:.3f}')\n",
    "    print(template.format(epoch+1, epoch_train_acc, epoch_train_loss, epoch_test_acc, epoch_test_loss))\n",
    "\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb37d44-4e75-4a91-9041-2ffe8f030b7e",
   "metadata": {},
   "source": [
    "## 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1b89f-7b7f-4903-bddb-500fe3f39514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(num_epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(epochs_range, train_acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, test_acc, label='Test Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_loss, label='Training Loss')\n",
    "plt.plot(epochs_range, test_loss, label='Test Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cea89f-dce5-4e40-9bf4-649809b33d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
