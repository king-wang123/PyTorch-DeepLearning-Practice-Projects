{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1822a7d-5698-4f4d-92ec-7f712fac9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6831f6f9-c45e-4e48-82e9-e8f9b02d207f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1816afb0-2565-4fbd-917d-54c4b391fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据数量260386\n",
      "                                             content  stars  label\n",
      "0  “我相信真正纯正的爱情能产生一个纾解死亡的阶段，所有的懦弱都出自于没有爱或爱得不彻底，这两者...      4      1\n",
      "1  太现实不是女人的错，不过年老色衰、中年危机了就不要自以为是，幻想重新寻找当年一口拒绝了的、虽...      4      1\n",
      "2                               跑吧，我们无力对抗，但也不能让他们得逞。      5      1\n",
      "3  我在同样变态的师傅手下呆了三年，祖宗十八代被骂了个遍，没空吃饭上厕所睡觉交朋友谈恋爱，脊椎侧...      5      1\n",
      "4                    还可以，是比较好的电影，但是又觉得和吕克贝松的巅峰状态差了好多      4      1\n"
     ]
    }
   ],
   "source": [
    "# 读取数据集\n",
    "data_path = '../datasets/chinese_movie_reviews/chinese_movie_reviews_datasets.jsonl'\n",
    "df = pd.read_json(data_path, orient='records', lines=True)\n",
    "print(f'数据数量{len(df)}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd708c-5e3a-487b-a8b9-026f91c9b685",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "1. 分词\n",
    "2. 训练Word2Vec：生成一个包含语料库中的每个词的向量空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da13e00-6a47-453a-8faf-fd551b5c8ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.301 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取停用词表\n",
    "with open('../datasets/chinese_movie_reviews/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 移除数字\n",
    "def remove_digit(text):\n",
    "     return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 分词处理\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "# 应用预处理\n",
    "def process_row(text):\n",
    "    text = remove_digit(text)\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fa5]\", \"\", text)  # 只保留汉字字符\n",
    "    text = tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(process_row) # 作者在这里大概用了2分钟\n",
    "\n",
    "# 计算每条文本的长度\n",
    "sentence_lengths = df[\"content\"].apply(lambda x: len(x.split()))  # 计算每条文本的词数（已经分词）\n",
    "# 计算最大长度和平均长度\n",
    "max_length = sentence_lengths.max()\n",
    "avg_length = sentence_lengths.mean()\n",
    "print(f\"最大文本长度：{max_length}\")\n",
    "print(f\"平均文本长度：{avg_length:.2f}\")\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0beb8-daab-4f56-bd8e-ca6391ad74d0",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "`Word2Vec训练很快，一分钟以内就能结束`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c737eff-6d74-4c26-9acb-fcdd69cb0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def prepare_data(df, vector_size, max_length=100):\n",
    "    # 将文本转换为词列表\n",
    "    texts = df['content'].apply(lambda x: x.split())\n",
    "\n",
    "    # 首先创建一个空的词汇表并添加 <PAD> 词索引为 0\n",
    "    vocab = {\"<PAD>\": 0}\n",
    "    \n",
    "    # 训练Word2Vec模型\n",
    "    w2v_model = Word2Vec(sentences=texts, vector_size=vector_size, window=8, min_count=4, workers=4)\n",
    "    \n",
    "    # 获取模型训练后生成的词汇表\n",
    "    vocab.update({k: v+1 for k, v in w2v_model.wv.key_to_index.items()})\n",
    "    \n",
    "    # 将文本转换为序列，如果词不在词汇表中，则用0表示\n",
    "    sequences = [[vocab.get(word, 0) for word in text] for text in texts]\n",
    "\n",
    "    # 对每个序列进行填充或截断\n",
    "    padded = [s[:max_length] + [0] * (max_length - len(s)) if len(s) < max_length \n",
    "              else s[:max_length] for s in sequences]\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(vocab), vector_size))\n",
    "    for word, i in vocab.items():\n",
    "        if word != \"<PAD>\":  # 确保 <PAD> 不会被赋予任何词向量\n",
    "            embedding_matrix[i] = w2v_model.wv[word]\n",
    "    \n",
    "    return np.array(padded), embedding_matrix, vocab\n",
    "\n",
    "vector_size=256\n",
    "padded, embedding_matrix, vocab = prepare_data(df, vector_size=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a2c11-6a54-4485-89c9-006af96d0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看词汇表的前几个词\n",
    "print(f'词汇表大小： {len(vocab)}')\n",
    "print(\"词汇表的一部分:\")\n",
    "for word, idx in list(vocab.items())[:5]:  # 查看前5个词\n",
    "    print(f\"词: {word}, 索引: {idx}\")\n",
    "\n",
    "# 查看嵌入矩阵中对应某个词的词向量\n",
    "word_to_check = '狗'  \n",
    "if word_to_check in vocab:\n",
    "    word_index = vocab[word_to_check]\n",
    "    word_vector = embedding_matrix[word_index]\n",
    "    print(f\"{word_to_check} 的词向量:\")\n",
    "    print(word_vector)\n",
    "else:\n",
    "    print(f\"词汇表中没有 {word_to_check} 这个词。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ae4b5-f74a-4aae-a3fa-b3b6fcee0fe2",
   "metadata": {},
   "source": [
    "#### 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798fe2c-4a7b-40c6-853e-659d2fcc8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "X = padded\n",
    "y = df['label'].values\n",
    "# stratify=df[\"label\"] 使得训练集和测试集中的标签分布是均匀\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=df[\"label\"])  \n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5b357-af62-4af1-b467-f71fd1c5ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练集和测试集的大小\n",
    "print(f\"训练集大小: {len(train_dataset)}\")\n",
    "print(f\"测试集大小: {len(test_dataset)}\")\n",
    "\n",
    "# 查看训练集和测试集的标签分布\n",
    "from collections import Counter\n",
    "\n",
    "train_labels_counter = Counter(y_train)\n",
    "test_labels_counter = Counter(y_test)\n",
    "print(f\"训练集标签分布: {train_labels_counter}\")\n",
    "print(f\"测试集标签分布: {test_labels_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d58254-08f2-43fc-a87e-23b4943cfafb",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559dadf-c087-4650-a68c-cee6efcfeef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        # 定义词嵌入层，使用 embedding_matrix 初始化\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = True # 确保嵌入层的参数可训练\n",
    "        \"\"\"\n",
    "        双向 LSTM 层：输入维度为 embedding_dim，输出维度为 hidden_dim。\n",
    "        batch_first=True : 输入张量的形状为 (batch_size, sequence_length)。\n",
    "        bidirectional=True : LSTM 会在两个方向上（正向和反向）处理输入序列，以捕捉更多上下文信息\n",
    "        (因为 LSTM 是双向的，它的输出将是两个隐藏层的连接, 所以实际输出维度为 hidden_dim * 2)\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,    # 输入特征的维度\n",
    "            hidden_dim,       # 隐藏状态的维度\n",
    "            num_layers=2,     # LSTM的层数\n",
    "            batch_first=True, # 输入和输出的张量的第一个维度是batch_size\n",
    "            bidirectional=True, # 使用双向LSTM\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # 2 classes for binary classification\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded) # 第二个返回值_是LSTM的隐藏状态和单元状态；lstm_out形状： (batch_size, sequence_length, hidden_dim * 2)\n",
    "        last_hidden = lstm_out[:, -1, :] # 选择每个批次中的最后一个时刻的输出，形状为 (batch_size, hidden_dim * 2)\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        fc1_out = F.relu(self.fc1(dropped))\n",
    "        # return self.fc2(fc1_out)\n",
    "        return self.fc2(self.dropout(fc1_out))\n",
    "\n",
    "\n",
    "model = TextClassifier(len(vocab) + 1, vector_size, embedding_matrix).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 初始化学习率调度器，每10个epoch将学习率衰减为原来的gamma倍\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7826e-5f40-474f-a02a-1d412d87c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看模型结构\n",
    "# 打印模型参数总数和可训练参数总数\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())  # 所有参数数量\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  # 需要训练的参数数量\n",
    "    print(f\"模型总参数数量: {total_params:,}\")\n",
    "    print(f\"模型可训练参数数量: {trainable_params:,}\")\n",
    "\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0e293-831e-4ace-8ab3-981be5be48f8",
   "metadata": {},
   "source": [
    "## 模型训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfde3d-bcc5-4bbc-b954-d3e576d1cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 训练函数\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 使用 tqdm 包裹数据加载器，显示进度条\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for texts, labels in progress_bar:\n",
    "        # 将数据移动到设备\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(texts)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  # 更新模型参数\n",
    "\n",
    "        # 统计指标\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # 更新进度条描述\n",
    "        progress_bar.set_postfix(lr=optimizer.param_groups[0]['lr'], loss=loss.item())\n",
    "        \n",
    "    scheduler.step()  # 更新学习率       \n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36267fd1-6b5e-4a5e-b9a7-e97652a9948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        for texts, labels in progress_bar:\n",
    "            # 将数据移动到设备\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(texts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # 统计指标\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # 更新进度条描述\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d90b49-560f-40b2-a1cd-d2fe798c8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "test_loss  = []\n",
    "test_acc   = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    epoch_train_loss, epoch_train_acc = train(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "    # 在测试集上评估\n",
    "    epoch_test_loss, epoch_test_acc = evaluate(test_loader, model, loss_fn)\n",
    "\n",
    "    train_acc.append(epoch_train_acc)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    test_acc.append(epoch_test_acc)\n",
    "    test_loss.append(epoch_test_loss)\n",
    "    # 打印训练和测试结果\n",
    "    template = ('Epoch:{:2d}, Train_acc:{:.1f}%, Train_loss:{:.3f}, Test_acc:{:.1f}%，Test_loss:{:.3f}')\n",
    "    print(template.format(epoch+1, epoch_train_acc, epoch_train_loss, epoch_test_acc, epoch_test_loss))\n",
    "\n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb37d44-4e75-4a91-9041-2ffe8f030b7e",
   "metadata": {},
   "source": [
    "## 结果可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1b89f-7b7f-4903-bddb-500fe3f39514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(num_epochs)\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(epochs_range, train_acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, test_acc, label='Test Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_loss, label='Training Loss')\n",
    "plt.plot(epochs_range, test_loss, label='Test Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cea89f-dce5-4e40-9bf4-649809b33d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
