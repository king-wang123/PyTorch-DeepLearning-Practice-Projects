{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59d773d-ccce-4953-ba62-2a7aba031e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203e402f-692c-4251-9c73-7959497f50a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ba5c86-ae56-428a-8f65-ef096e60bf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据数量260386\n",
      "                                             content  stars  label\n",
      "0  “我相信真正纯正的爱情能产生一个纾解死亡的阶段，所有的懦弱都出自于没有爱或爱得不彻底，这两者...      4      1\n",
      "1  太现实不是女人的错，不过年老色衰、中年危机了就不要自以为是，幻想重新寻找当年一口拒绝了的、虽...      4      1\n",
      "2                               跑吧，我们无力对抗，但也不能让他们得逞。      5      1\n",
      "3  我在同样变态的师傅手下呆了三年，祖宗十八代被骂了个遍，没空吃饭上厕所睡觉交朋友谈恋爱，脊椎侧...      5      1\n",
      "4                    还可以，是比较好的电影，但是又觉得和吕克贝松的巅峰状态差了好多      4      1\n"
     ]
    }
   ],
   "source": [
    "# 读取数据集\n",
    "data_path = '../datasets/chinese_movie_reviews/chinese_movie_reviews_datasets.jsonl'\n",
    "df = pd.read_json(data_path, orient='records', lines=True)\n",
    "print(f'数据数量{len(df)}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60adc1-e45c-483e-8acc-a0271c3e7ce4",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "\n",
    "1. 分词\n",
    "2. 构建词汇表 ： 将每个词映射到一个唯一的数字索引\n",
    "3. 构造 Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95335b2-a050-4ce7-86cf-26d8603e26a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\PC\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.294 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大文本长度：166\n",
      "平均文本长度：21.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>stars</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>相信 真正 纯正 爱情 能 产生 纾解 死亡 阶段 所有 懦弱 出自于 爱 或 爱 得 彻底...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>太 现实 不是 女人 错 年老色衰 中年 危机 不要 自以为是 幻想 重新 寻找 当年 一口...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>跑 我们 无力 对抗 但 不能 得逞</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>同样 变态 师傅 手下 呆 三年 祖宗 十八代 骂 个 遍 没空 吃饭 厕所 睡觉 交朋友 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>还 比较 好 觉得 吕克贝 松 巅峰状态 差 好多</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  stars  label\n",
       "0  相信 真正 纯正 爱情 能 产生 纾解 死亡 阶段 所有 懦弱 出自于 爱 或 爱 得 彻底...      4      1\n",
       "1  太 现实 不是 女人 错 年老色衰 中年 危机 不要 自以为是 幻想 重新 寻找 当年 一口...      4      1\n",
       "2                                 跑 我们 无力 对抗 但 不能 得逞      5      1\n",
       "3  同样 变态 师傅 手下 呆 三年 祖宗 十八代 骂 个 遍 没空 吃饭 厕所 睡觉 交朋友 ...      5      1\n",
       "4                          还 比较 好 觉得 吕克贝 松 巅峰状态 差 好多      4      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取停用词表\n",
    "with open('../datasets/chinese_movie_reviews/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# 移除数字\n",
    "def remove_digit(text):\n",
    "     return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# 分词处理\n",
    "def tokenize(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
    "\n",
    "# 应用预处理\n",
    "def process_row(text):\n",
    "    text = remove_digit(text)\n",
    "    text = re.sub(r\"[^\\u4e00-\\u9fa5]\", \"\", text)  # 只保留汉字字符\n",
    "    text = tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "df[\"content\"] = df[\"content\"].apply(process_row) # 作者在这里大概用了2分钟\n",
    "\n",
    "# 计算每条文本的长度\n",
    "sentence_lengths = df[\"content\"].apply(lambda x: len(x.split()))  # 计算每条文本的词数（已经分词）\n",
    "# 计算最大长度和平均长度\n",
    "max_length = sentence_lengths.max()\n",
    "avg_length = sentence_lengths.mean()\n",
    "print(f\"最大文本长度：{max_length}\")\n",
    "print(f\"平均文本长度：{avg_length:.2f}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000bbdca-d906-44fa-a1b8-2a11d913d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "260386lines [00:01, 156910.16lines/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 209859\n",
      "<unk>: 0\n",
      "<pad>: 1\n",
      "看: 2\n",
      "人: 3\n",
      "但: 4\n",
      "好: 5\n",
      "这: 6\n",
      "还是: 7\n",
      "还: 8\n",
      "啊: 9\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 1. 分词\n",
    "def yield_tokens(text_iter):\n",
    "    for text in text_iter:\n",
    "        yield text.split()  # 返回分词后的结果\n",
    "\n",
    "# 2. 创建基础词汇表\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df[\"content\"]))\n",
    "\n",
    "# 输出词汇表的大小\n",
    "print(f\"词汇表大小: {len(vocab)}\")\n",
    "\n",
    "# 查看词汇表中的前 10 个词汇及其索引\n",
    "for i, (word, idx) in enumerate(vocab.stoi.items()):\n",
    "    if i >= 10:  # 只查看前 10 个\n",
    "        break\n",
    "    print(f\"{word}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f336ea-4fac-4b29-ba92-9230cae7d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 32  # 可以自定义最大长度\n",
    "\n",
    "def pad_truncate(tokens):\n",
    "    if len(tokens) > MAX_LENGTH:\n",
    "        return tokens[:MAX_LENGTH]  # 如果超过最大长度，裁剪\n",
    "    else:\n",
    "        return tokens + [vocab[\"<pad>\"]] * (MAX_LENGTH - len(tokens))  # 不足则填充\n",
    "\n",
    "def text_pipeline(text):\n",
    "    tokens = [vocab[token] for token in text]  # 通过词汇表将词语转换为索引\n",
    "    return pad_truncate(tokens)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义数据集类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, text_pipeline):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.text_pipeline = text_pipeline\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        return torch.tensor(self.text_pipeline(text)), torch.tensor(label)\n",
    "\n",
    "# 将数据转换为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 按 99% 训练集和 1% 测试集来划分(数据集比较大即使是1%也有2w6千条数据\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"content\"], df[\"label\"], test_size=0.10, random_state=42, stratify=df[\"label\"]) # stratify=df[\"label\"] 使得训练集和测试集中的标签分布是均匀\n",
    "\n",
    "# 创建训练集和测试集的 Dataset\n",
    "train_dataset = TextDataset(train_texts, train_labels, text_pipeline)\n",
    "test_dataset = TextDataset(test_texts, test_labels, text_pipeline)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0e5f4-c49c-4d8c-8b8a-6fe30728f1ae",
   "metadata": {},
   "source": [
    "### 定义模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d40968-323f-4732-add4-0874df720975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (embedding): Embedding(209859, 128)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "模型总参数数量: 28,055,938\n",
      "模型可训练参数数量: 28,055,938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\develop\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义模型架构，使用 Transformer 模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_encoder_layers, hidden_dim, output_dim, max_len=MAX_LENGTH):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_encoding = nn.Parameter(torch.zeros(1, max_len, embed_size))\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.position_encoding[:, :seq_len, :]\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # 聚合所有位置的输出\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 模型初始化\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 128\n",
    "num_heads = 8\n",
    "num_encoder_layers = 6\n",
    "hidden_dim = 512\n",
    "output_dim = 2\n",
    "\n",
    "model = TransformerModel(vocab_size, embed_size, num_heads, num_encoder_layers, hidden_dim, output_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "# 查看模型结构\n",
    "# 打印模型参数总数和可训练参数总数\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())  # 所有参数数量\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  # 需要训练的参数数量\n",
    "    print(f\"模型总参数数量: {total_params:,}\")\n",
    "    print(f\"模型可训练参数数量: {trainable_params:,}\")\n",
    "\n",
    "print(model)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee3dbf-2154-4b8f-8490-63480b5d69f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                               | 0/7324 [00:00<?, ?it/s]D:\\develop\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6943, Train Accuracy: 0.5009 | Validation Loss: 0.6932, Validation Accuracy: 0.5000\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.6933, Train Accuracy: 0.5020 | Validation Loss: 0.6932, Validation Accuracy: 0.5000\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.6933, Train Accuracy: 0.5004 | Validation Loss: 0.6933, Validation Accuracy: 0.5000\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.6933, Train Accuracy: 0.4992 | Validation Loss: 0.6932, Validation Accuracy: 0.5000\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.6933, Train Accuracy: 0.5003 | Validation Loss: 0.6933, Validation Accuracy: 0.5000\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|████████▋                                               | 1133/7324 [00:33<03:00, 34.39it/s, loss=0.694]"
     ]
    }
   ],
   "source": [
    "# 损失函数与优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# # 设置学习率调度器， 每 5 个 epoch 调整一次学习率\n",
    "# scheduler = StepLR(optimizer, step_size=2, gamma=0.2)\n",
    "\n",
    "# 训练与验证\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        for texts, labels in progress_bar:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_acc += (preds == labels).sum().item()\n",
    "            \n",
    "            # 更新进度条描述\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # # 更新学习率\n",
    "        # scheduler.step()  # 每个 epoch 后更新一次学习率\n",
    "        # print(f\"当前学习率: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = total_acc / len(train_loader.dataset)\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss, val_acc = evaluate_model(val_loader, model, criterion)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} | '\n",
    "      f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f} | '\n",
    "      f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        \n",
    "def evaluate_model(loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(loader, desc=\"Evaluating\", leave=False)\n",
    "        for texts, labels in progress_bar:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total_acc += (preds == labels).sum().item()\n",
    "            \n",
    "            # 更新进度条描述\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    return total_loss / len(loader), total_acc / len(loader.dataset)\n",
    "\n",
    "# 开始训练\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a090032-3814-4716-a14d-cf0ba1552e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
